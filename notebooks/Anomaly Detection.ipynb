{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of anomalous timeseries\n",
    "\n",
    "In Stackstate we are challenged with a number of problems from IT operations that we are solving using Machine Learning techniques.\n",
    "\n",
    "One of the problems that is very important is classification of timeseries for further automatic problem detection at scale.\n",
    "Usually the metrics are coming from different sources and monitoring tools. The metrics typically are [\"Golden signals\"](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/#xref_monitoring_golden-signals), resource utilization (CPU/RAM/Disk), business KPIs (Latency).\n",
    "\n",
    "In this notebook I'd like to give an introduction of how classification of time series can be performed. In order to do that I formulate the following questions.\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "1. How does anomaly look like and how often it occurs? \n",
    "2. Can anomaly be detected using a classifier?\n",
    "3. How to overcome problems with training for anomaly detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including parent path\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/anaconda3/envs/time-series/lib/python36.zip', '/anaconda3/envs/time-series/lib/python3.6', '/anaconda3/envs/time-series/lib/python3.6/lib-dynload', '', '/anaconda3/envs/time-series/lib/python3.6/site-packages', '/anaconda3/envs/time-series/lib/python3.6/site-packages/IPython/extensions', '/Users/konst/.ipython', '/Users/konst/stackvista/time-series']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date, timedelta as td\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unittest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from preprocessing.test_common import make_labels, make_series\n",
    "from preprocessing.helper import TimeseriesHelper\n",
    "from preprocessing.preprocessing import TimeSeriesPreprocessor\n",
    "from preprocessing.feature_engineering import TimeSeriesFeatureEngineering\n",
    "from models.xgboost import XGBoostModel\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters()\n",
    "warnings.filterwarnings('ignore')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data\n",
    "\n",
    "I have chosen the NAB dataset for anomaly research because it is publicly available dataset that contains a representative set of timeseries with labeled anomalies.\n",
    "\n",
    "It is very important that the anomalies are labeled by the experts as they provide necessary context and establish necessary casual relationship between change in metric - a signal and corresponding issue - symptom. \n",
    "\n",
    "If the time series is not labled then it is difficult to conclude if the change in the signal that reached a threshold or a frequency change indicating typical changepoint or actual problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/NAB/realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b3f168be7735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m      \u001b[0;34m\"realKnownCause/ec2_request_latency_system_failure.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m      \u001b[0;34m\"realKnownCause/machine_temperature_system_failure.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m      \"realKnownCause/ambient_temperature_system_failure.csv\"])\n\u001b[0m",
      "\u001b[0;32m~/stackvista/time-series/preprocessing/helper.py\u001b[0m in \u001b[0;36mload_multiple_series\u001b[0;34m(self, metric_files)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_labeled_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric_file\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/stackvista/time-series/preprocessing/helper.py\u001b[0m in \u001b[0;36mload_labeled_series\u001b[0;34m(self, series_path)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts_variable\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtimestamp\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         '''\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"data/NAB/{series_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/time-series/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/time-series/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/time-series/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/time-series/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/time-series/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/NAB/realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv' does not exist"
     ]
    }
   ],
   "source": [
    "helper = TimeseriesHelper()\n",
    "\n",
    "metric_map = helper.load_multiple_series(\n",
    "    [\"realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv\", \n",
    "     \"realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv\",\n",
    "     \"realAWSCloudwatch/ec2_disk_write_bytes_c0d644.csv\",\n",
    "     \"realAWSCloudwatch/grok_asg_anomaly.csv\",\n",
    "     \"realTweets/Twitter_volume_AMZN.csv\",\n",
    "     \"realTweets/Twitter_volume_UPS.csv\",\n",
    "     \"realTraffic/TravelTime_387.csv\",\n",
    "     \"realKnownCause/ec2_request_latency_system_failure.csv\",\n",
    "     \"realKnownCause/machine_temperature_system_failure.csv\",\n",
    "     \"realKnownCause/ambient_temperature_system_failure.csv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How does anomaly look like and how often it occurs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now examine the number of NAB metrics we preselected, plot them with corresponding labels and give the definition of anomaly according to what the experts thought the anomaly was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "helper.plot_metrics(metric_map, figsize=(18, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Definition\n",
    "\n",
    "From the charts above it is clear that the anomaly is:\n",
    "- Sudden metric value spike or collapse\n",
    "- Sudden change in variance\n",
    "- Sudden level shift\n",
    "- Any combination of one of the mentioned above\n",
    "\n",
    "Anomalies are quite rare. I foresee issues due to class disbalance where it is easier for a model to always say \"No Anomaly\" and still get 99.99% accuracy.\n",
    "\n",
    "Note that we are analyzing the anomalies pretty much without the context, observing the metric data itself. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Before extracting features from time series it is always a good practice to perform data cleaning and preprocessing.\n",
    "\n",
    "I will execute the following preprocessing steps:\n",
    "1. Cleaning from missing values\n",
    "2. Aggregating into 5 minute buckets.\n",
    "3. If after aggregation there are missing values they are imputed using backfill and forwardfill methods.\n",
    "\n",
    "The result dataset is ready for feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_request_latency_system_failure_file = \"realKnownCause/ec2_request_latency_system_failure.csv\" # bad\n",
    "ec2_cpu_utilization_failure_file = \"realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv\" \n",
    "Twitter_volume_AMZN_file = \"realTweets/Twitter_volume_AMZN.csv\" # good\n",
    "machine_temperature_system_failure_file = \"realKnownCause/machine_temperature_system_failure.csv\" # good\n",
    "\n",
    "df1 = metric_map[ec2_request_latency_system_failure_file]\n",
    "df2 = metric_map[ec2_cpu_utilization_failure_file]\n",
    "df3 = metric_map[Twitter_volume_AMZN_file]\n",
    "df4 = metric_map[machine_temperature_system_failure_file]\n",
    "\n",
    "train = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "input_variables = ['y']\n",
    "output_variable = 'label'\n",
    "\n",
    "preprocessor = TimeSeriesPreprocessor(\n",
    "    window_size_seconds = 7200 * 3,\n",
    "    window_shift = 300,\n",
    "    horizon_shift_seconds = 3600,\n",
    "    probe_period_seconds = 300,\n",
    "    scaler = MinMaxScaler())\n",
    "\n",
    "series = preprocessor.prepare_series(train,\n",
    "        input_vars = input_variables, output_vars = [output_variable],\n",
    "        numeric_vars = input_variables, auto_impute = input_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic Feature Engineering\n",
    "\n",
    "Feature extraction and engineering is laborious process that takes considerable amount of time therefore I'd like to perform it automatically. \n",
    "\n",
    "For this purpose I will use [tsfresh](https://tsfresh.readthedocs.io/en/latest/) library that does automatic feature extraction of around [700 features](https://tsfresh.readthedocs.io/en/latest/text/list_of_features.html) and then execute statistical tests to select only those features that are relevant for classification task. For more details on how it does that I am forwarding the reader to the documentation on [feature extraction](https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html) and [feature filtering](https://tsfresh.readthedocs.io/en/latest/text/feature_filtering.html)\n",
    "\n",
    "We extract and select the number of features from rolling window of 20 points which corresponds to ~ 100 minutes of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering = TimeSeriesFeatureEngineering(\n",
    "      x_columns = input_variables,\n",
    "      roll_shift = 20,\n",
    "      ts_variable = 'timestamp',\n",
    "      y_column = output_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = feature_engineering.make_features(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since feature engineering is quite CPU and RAM intensive process I dont want to repeat it often during experimenting and I am happy to reuse this result and therefore save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset is not None:\n",
    "    dataset.to_csv(\"./extracted_features.csv\")\n",
    "else:\n",
    "    dataset = pd.read_csv(\"./df4_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am splitting the dataset to train/validation/test sets as always in such cases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_ratio = 0.2\n",
    "train_valid_split_ratio = 0.2\n",
    "seed = 42\n",
    "\n",
    "input_features = list(dataset.columns)\n",
    "input_features.remove(output_variable)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[input_features], dataset[output_variable], test_size=train_test_split_ratio, random_state = seed)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = train_valid_split_ratio, random_state = seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training Gradient Boosting tree model\n",
    "\n",
    "I am going to train Gradient Boosting Tree as a classifier. This type of model has very good performance although it is also prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBoostModel()\n",
    "\n",
    "model.fit(X_train.as_matrix(), y_train.as_matrix(), X_valid.as_matrix(), y_valid.as_matrix())\n",
    "\n",
    "Y_train_pred = model.predict(X_train.as_matrix())\n",
    "helper.print_results(\"Training\", helper.evaluate(y_train, Y_train_pred))\n",
    "\n",
    "Y_valid_pred = model.predict(X_valid.as_matrix())\n",
    "helper.print_results(\"Validation\", helper.evaluate(y_valid, Y_valid_pred))\n",
    "\n",
    "y_test_pred = model.predict(X_test.as_matrix())\n",
    "helper.print_results(\"Test\", helper.evaluate(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad considering the fact that there is only ~1 % positive samples \n",
    "\n",
    "The result overview over all 4 series is given below:\n",
    "\n",
    "| F1 scores                   | training | validation | test |\n",
    "|-----------------------------|----------|------------|------|\n",
    "| request latency failure     | 0.98     | 1.0        | 1.0  |\n",
    "| cpu utilization failure     | 0.68     | 0.62       | 0.47 |\n",
    "| twitter volume AMZN         | 0.83     | 0.74       | 0.61 |\n",
    "| Machine temperature failure | 0.60     | 0.60       | 0.5  |\n",
    "\n",
    "It seems that in certain cases the classifier a bit overfits the data.\n",
    "It is also noticible that performance is not great on some datasets and from confusion matrix it looks like classifier tends to infer \"not a anomaly\" quite often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Class Disbalance\n",
    "\n",
    "As it was expected the anomalies are quite rare therefore one of the big challenges in machine learning on anomalous time series is how to train decent classifier with only a couple anomalous samples.\n",
    "\n",
    "In order to fix the class disblance I am going to artificially increase the number of anomalous samples by using [Synthetic Minority Over-sampling Technique](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html). This will allow to balance the number of positive and negative samples and I avoid cheap wins like always detect \"Not anomaly\".\n",
    "\n",
    "The code and results of those experiments are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"class distribution in original training data.\\n\", y_train.groupby(by=lambda v: y_train.loc[v]).count())\n",
    "X_upsampled, y_upsampled = feature_engineering.class_imbalance_fix(X_train, y_train)\n",
    "elements, counts_elements = np.unique(y_upsampled, return_counts=True)\n",
    "print(\"class distribution in upsampled training data.\")\n",
    "print(elements)\n",
    "print(counts_elements)\n",
    "\n",
    "model = XGBoostModel()\n",
    "model.fit(X_upsampled, y_upsampled, X_valid.as_matrix(), y_valid.as_matrix())\n",
    "\n",
    "Y_train_pred = model.predict(X_train.as_matrix())\n",
    "helper.print_results(\"Training\", helper.evaluate(y_train, Y_train_pred))\n",
    "\n",
    "Y_valid_pred = model.predict(X_valid.as_matrix())\n",
    "helper.print_results(\"Validation\", helper.evaluate(y_valid, Y_valid_pred))\n",
    "\n",
    "y_test_pred = model.predict(X_test.as_matrix())\n",
    "helper.print_results(\"Test\", helper.evaluate(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like with the increasing of the number of positive samples using SMOTE our model perform much better.\n",
    "\n",
    "| F1 scores                   | training | validation | test |\n",
    "|-----------------------------|----------|------------|------|\n",
    "| request latency failure     | 0.98     | 1.0        | 1.0  |\n",
    "| cpu utilization failure     | 0.94     | 0.66       | 0.8  |\n",
    "| twitter volume AMZN         | 0.61     | 0.39       | 0.59 |\n",
    "| Machine temperature failure | 0.60     | 0.60       | 0.5  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Futher increase of accuracy with the window size increase\n",
    "\n",
    "It is possible to increase accuracy further by using bigger windows. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
